#-------1---------DCA_SDC_CAN_EfficientNetB0模型消融
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
from tensorflow.keras.mixed_precision import LossScaleOptimizer
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.profiler.model_analyzer import profile
from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
import time
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score,confusion_matrix
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 启用混合精度训练
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# 设置路径与参数
train_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/train"
val_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/val"
test_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/test"
input_shape = (224, 224, 3)
num_classes = 10
batch_size = 32
epochs = 35

# 定义自定义模块
class DynamicChannelAttention(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.global_avg_pool = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(self.reduction_ratio, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.fc2 = layers.Dense(input_shape[-1], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))

    def call(self, inputs):
        avg_pool = self.global_avg_pool(inputs)
        attention = self.fc1(avg_pool)
        attention = self.fc2(attention)
        attention = layers.Reshape((1, 1, -1))(attention)
        return inputs * attention

class SeparableDilatedConvBlock(layers.Layer):
    def __init__(self, filters, dilation_rate):
        super().__init__()
        self.conv1 = layers.SeparableConv2D(filters, (3, 3), dilation_rate=dilation_rate, padding='same', activation='swish')
        self.conv2 = layers.Conv2D(filters, (1, 1), padding='same', activation='swish')

    def call(self, inputs):
        x = self.conv1(inputs)
        return self.conv2(x)

class CANBlock(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(input_shape[-1] // self.reduction_ratio, kernel_size=(3, 3), padding='same', activation='swish')
        self.conv2 = layers.Conv2D(input_shape[-1], kernel_size=(1, 1), padding='same', activation='sigmoid')

    def call(self, inputs):
        x = self.conv1(inputs)
        attention_map = self.conv2(x)
        return inputs * attention_map

# 构建改进EfficientNetB0模型
def create_improved_efficientnet(input_shape=(224, 224, 3), num_classes=10, use_dca=True, use_sdc=True, use_can=True):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    base_model.trainable = True  # 微调
    model_input = layers.Input(shape=input_shape)
    x = base_model(model_input)
    if use_dca:
        x = DynamicChannelAttention()(x)
    if use_sdc:
        x = SeparableDilatedConvBlock(64, dilation_rate=(2, 2))(x)
    if use_can:
        x = CANBlock()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(1024, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # 输出层需显式指定为 float32
    model = models.Model(inputs=model_input, outputs=output)
    return model

# 消融实验的辅助函数：评估指标
def evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs):
    # 模型评估
    print(f"Evaluating {model_name}...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    
    # 精度、精确率、召回率、F1分数
    y_true = test_generator.classes
    y_pred = np.argmax(model.predict(test_generator), axis=-1)
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # 绘制混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()
    
    # 获取模型的FLOPs
    input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32)]  # 输入签名
    graph = tf.function(model).get_concrete_function(*input_signature)
    opts = ProfileOptionBuilder.float_operation()  # 配置FLOPs计算选项
    opts['output'] = 'none'
    flops = profile(graph.graph, options=opts).total_float_ops  # 获取FLOPs
    flops_g = flops / 1e9  # 将FLOPs换算成G FLOPs
    # 推理时间 (ms)
    start_time = time.time()
    # 进行多次推理，取平均时间
    n_iterations = 100  # 推理次数
    for _ in range(n_iterations):
        model.predict(test_generator, verbose=0)
    inference_time = (time.time() - start_time) / n_iterations 
    # 结果存储
    results = {
        'model_name': model_name,
        'accuracy': f"{test_acc * 100:.2f}%",
        'precision': f"{precision * 100:.2f}%",
        'recall': f"{recall * 100:.2f}%",
        'f1_score': f"{f1 * 100:.2f}%",
        'flops': flops_g,
        'params': model.count_params(),
        'inference_time': f"{inference_time:.2f}s",
    }
    # 创建结果表格
    results_df = pd.DataFrame([results])
    display(results_df)
    return results_df
    
# 设置消融实验的模型配置
def create_ablation_models(input_shape=(224, 224, 3), num_classes=10):
    models_config = {
        'DCA_SDC_CAN_EfficientNetB0': create_improved_efficientnet(input_shape=input_shape, num_classes=num_classes)   
    }
    return models_config

# 数据预处理
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)

reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.4, min_lr=1e-6, verbose=1)

# 获取消融实验的模型配置
models_config = create_ablation_models(input_shape=input_shape, num_classes=10)

# 训练并评估每个模型
historis = {}
results = {}
for model_name, model in models_config.items():
    print(f"Training model: {model_name}")
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='categorical_crossentropy', 
                  metrics=['accuracy'])
    # 训练模型（使用对应的回调）
    history = model.fit(train_generator, 
                        epochs=epochs, 
                        validation_data=val_generator, 
                        callbacks=[reduce_lr])
    # 保存训练历史
    historis[model_name] = history.history
    results_df = evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs)
    results[model_name] = results_df
    '''
    # 获取测试集上的准确率
    test_accuracy = results_df.loc[0, 'accuracy']  # 从评估结果中提取测试集准确率
    test_accuracy = float(test_accuracy.strip('%')) / 100  # 将百分比转换为小数形式
    if model_name == 'DCA_SDC_CAN_EfficientNetB0'and test_accuracy <= 0.9950:
        print("Test accuracy is below threshold. Stopping training and exiting.")
        sys.exit()  # 退出程序
    '''
# 转换为 DataFrame，并展示
results_df = pd.concat(results.values(), ignore_index=True)
display(results_df)






#--------2-----------DCA_SDC_EfficientNetB0
# 导入必要库
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
from tensorflow.keras.mixed_precision import LossScaleOptimizer
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.profiler.model_analyzer import profile
from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
import time
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score,confusion_matrix
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 启用混合精度训练
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# 设置路径与参数
train_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/train"
val_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/val"
test_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/test"
input_shape = (224, 224, 3)
num_classes = 10
batch_size = 32
epochs = 35

# 定义自定义模块
class DynamicChannelAttention(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.global_avg_pool = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(self.reduction_ratio, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.fc2 = layers.Dense(input_shape[-1], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))

    def call(self, inputs):
        avg_pool = self.global_avg_pool(inputs)
        attention = self.fc1(avg_pool)
        attention = self.fc2(attention)
        attention = layers.Reshape((1, 1, -1))(attention)
        return inputs * attention

class SeparableDilatedConvBlock(layers.Layer):
    def __init__(self, filters, dilation_rate):
        super().__init__()
        self.conv1 = layers.SeparableConv2D(filters, (3, 3), dilation_rate=dilation_rate, padding='same', activation='swish')
        self.conv2 = layers.Conv2D(filters, (1, 1), padding='same', activation='swish')

    def call(self, inputs):
        x = self.conv1(inputs)
        return self.conv2(x)

class CANBlock(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(input_shape[-1] // self.reduction_ratio, kernel_size=(3, 3), padding='same', activation='swish')
        self.conv2 = layers.Conv2D(input_shape[-1], kernel_size=(1, 1), padding='same', activation='sigmoid')

    def call(self, inputs):
        x = self.conv1(inputs)
        attention_map = self.conv2(x)
        return inputs * attention_map

# 构建改进EfficientNetB0模型
def create_improved_efficientnet(input_shape=(224, 224, 3), num_classes=10, use_dca=True, use_sdc=True, use_can=True):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    base_model.trainable = True  # 微调
    model_input = layers.Input(shape=input_shape)
    x = base_model(model_input)
    if use_dca:
        x = DynamicChannelAttention()(x)
    if use_sdc:
        x = SeparableDilatedConvBlock(64, dilation_rate=(2, 2))(x)
    if use_can:
        x = CANBlock()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(1024, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # 输出层需显式指定为 float32
    model = models.Model(inputs=model_input, outputs=output)
    return model

# 消融实验的辅助函数：评估指标
def evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs):
    # 模型评估
    print(f"Evaluating {model_name}...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    
    # 精度、精确率、召回率、F1分数
    y_true = test_generator.classes
    y_pred = np.argmax(model.predict(test_generator), axis=-1)
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # 绘制混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()
    
    # 获取模型的FLOPs
    input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32)]  # 输入签名
    graph = tf.function(model).get_concrete_function(*input_signature)
    opts = ProfileOptionBuilder.float_operation()  # 配置FLOPs计算选项
    opts['output'] = 'none'
    flops = profile(graph.graph, options=opts).total_float_ops  # 获取FLOPs
    flops_g = flops / 1e9  # 将FLOPs换算成G FLOPs
    # 推理时间 (ms)
    start_time = time.time()
    # 进行多次推理，取平均时间
    n_iterations = 100  # 推理次数
    for _ in range(n_iterations):
        model.predict(test_generator, verbose=0)
    inference_time = (time.time() - start_time) / n_iterations 
    # 结果存储
    results = {
        'model_name': model_name,
        'accuracy': f"{test_acc * 100:.2f}%",
        'precision': f"{precision * 100:.2f}%",
        'recall': f"{recall * 100:.2f}%",
        'f1_score': f"{f1 * 100:.2f}%",
        'flops': flops_g,
        'params': model.count_params(),
        'inference_time': f"{inference_time:.2f}s",
    }
    # 创建结果表格
    results_df = pd.DataFrame([results])
    display(results_df)
    return results_df
    
# 设置消融实验的模型配置
def create_ablation_models(input_shape=(224， 224， 3), num_classes=10):
    models_config = {
        'DCA_SDC_EfficientNetB0': create_improved_efficientnet(input_shape=input_shape, num_classes=num_classes, use_can=False)    
    }
    return models_config

# 数据预处理
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)

reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.4, min_lr=1e-6, verbose=1)

# 获取消融实验的模型配置
models_config = create_ablation_models(input_shape=input_shape, num_classes=10)

# 训练并评估每个模型
historis = {}
results = {}
for model_name, model 在 models_config.items():
    print(f"Training model: {model_name}")
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='categorical_crossentropy', 
                  metrics=['accuracy'])
    # 训练模型（使用对应的回调）
    history = model.fit(train_generator, 
                        epochs=epochs, 
                        validation_data=val_generator, 
                        callbacks=[reduce_lr])
    # 保存训练历史
    historis[model_name] = history.history
    results_df = evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs)
    results[model_name] = results_df
    '''
    # 获取测试集上的准确率
    test_accuracy = results_df.loc[0, 'accuracy']  # 从评估结果中提取测试集准确率
    test_accuracy = float(test_accuracy.strip('%')) / 100  # 将百分比转换为小数形式
    if model_name == 'DCA_SDC_CAN_EfficientNetB0'and test_accuracy <= 0.9950:
        print("Test accuracy is below threshold. Stopping training and exiting.")
        sys.exit()  # 退出程序
    '''
# 转换为 DataFrame，并展示
results_df = pd.concat(results.values(), ignore_index=True)
display(results_df)






#---------3----------DCA_CAN_EfficientNetB0
# 导入必要库
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
from tensorflow.keras.mixed_precision import LossScaleOptimizer
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.profiler.model_analyzer import profile
from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
import time
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score,confusion_matrix
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 启用混合精度训练
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# 设置路径与参数
train_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/train"
val_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/val"
test_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/test"
input_shape = (224, 224, 3)
num_classes = 10
batch_size = 32
epochs = 35

# 定义自定义模块
class DynamicChannelAttention(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.global_avg_pool = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(self.reduction_ratio, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.fc2 = layers.Dense(input_shape[-1], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))

    def call(self, inputs):
        avg_pool = self.global_avg_pool(inputs)
        attention = self.fc1(avg_pool)
        attention = self.fc2(attention)
        attention = layers.Reshape((1, 1, -1))(attention)
        return inputs * attention

class SeparableDilatedConvBlock(layers.Layer):
    def __init__(self, filters, dilation_rate):
        super().__init__()
        self.conv1 = layers.SeparableConv2D(filters, (3, 3), dilation_rate=dilation_rate, padding='same', activation='swish')
        self.conv2 = layers.Conv2D(filters, (1, 1), padding='same', activation='swish')

    def call(self, inputs):
        x = self.conv1(inputs)
        return self.conv2(x)

class CANBlock(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(input_shape[-1] // self.reduction_ratio, kernel_size=(3, 3), padding='same', activation='swish')
        self.conv2 = layers.Conv2D(input_shape[-1], kernel_size=(1, 1), padding='same', activation='sigmoid')

    def call(self, inputs):
        x = self.conv1(inputs)
        attention_map = self.conv2(x)
        return inputs * attention_map

# 构建改进EfficientNetB0模型
def create_improved_efficientnet(input_shape=(224, 224, 3), num_classes=10, use_dca=True, use_sdc=True, use_can=True):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    base_model.trainable = True  # 微调
    model_input = layers.Input(shape=input_shape)
    x = base_model(model_input)
    if use_dca:
        x = DynamicChannelAttention()(x)
    if use_sdc:
        x = SeparableDilatedConvBlock(64, dilation_rate=(2, 2))(x)
    if use_can:
        x = CANBlock()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(1024, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # 输出层需显式指定为 float32
    model = models.Model(inputs=model_input, outputs=output)
    return model

# 消融实验的辅助函数：评估指标
def evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs):
    # 模型评估
    print(f"Evaluating {model_name}...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    
    # 精度、精确率、召回率、F1分数
    y_true = test_generator.classes
    y_pred = np.argmax(model.predict(test_generator), axis=-1)
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # 绘制混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()
    
    # 获取模型的FLOPs
    input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32)]  # 输入签名
    graph = tf.function(model).get_concrete_function(*input_signature)
    opts = ProfileOptionBuilder.float_operation()  # 配置FLOPs计算选项
    opts['output'] = 'none'
    flops = profile(graph.graph, options=opts).total_float_ops  # 获取FLOPs
    flops_g = flops / 1e9  # 将FLOPs换算成G FLOPs
    # 推理时间 (ms)
    start_time = time.time()
    # 进行多次推理，取平均时间
    n_iterations = 100  # 推理次数
    for _ in range(n_iterations):
        model.predict(test_generator, verbose=0)
    inference_time = (time.time() - start_time) / n_iterations 
    # 结果存储
    results = {
        'model_name': model_name,
        'accuracy': f"{test_acc * 100:.2f}%",
        'precision': f"{precision * 100:.2f}%",
        'recall': f"{recall * 100:.2f}%",
        'f1_score': f"{f1 * 100:.2f}%",
        'flops': flops_g,
        'params': model.count_params(),
        'inference_time': f"{inference_time:.2f}s",
    }
    # 创建结果表格
    results_df = pd.DataFrame([results])
    display(results_df)
    return results_df
    
# 设置消融实验的模型配置
def create_ablation_models(input_shape=(224， 224， 3), num_classes=10):
    models_config = {
        'DCA_CAN_EfficientNetB0': create_improved_efficientnet(input_shape=input_shape, num_classes=num_classes, use_sdc=False)   
    }
    return models_config

# 数据预处理
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)

reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.4, min_lr=1e-6, verbose=1)

# 获取消融实验的模型配置
models_config = create_ablation_models(input_shape=input_shape, num_classes=10)

# 训练并评估每个模型
historis = {}
results = {}
for model_name, model in models_config.items():
    print(f"Training model: {model_name}")
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='categorical_crossentropy', 
                  metrics=['accuracy'])
    # 训练模型（使用对应的回调）
    history = model.fit(train_generator, 
                        epochs=epochs, 
                        validation_data=val_generator, 
                        callbacks=[reduce_lr])
    # 保存训练历史
    historis[model_name] = history.history
    results_df = evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs)
    results[model_name] = results_df
    '''
    # 获取测试集上的准确率
    test_accuracy = results_df.loc[0, 'accuracy']  # 从评估结果中提取测试集准确率
    test_accuracy = float(test_accuracy.strip('%')) / 100  # 将百分比转换为小数形式
    if model_name == 'DCA_SDC_CAN_EfficientNetB0'and test_accuracy <= 0.9950:
        print("Test accuracy is below threshold. Stopping training and exiting.")
        sys.exit()  # 退出程序
    '''
# 转换为 DataFrame，并展示
results_df = pd.concat(results.values(), ignore_index=True)
display(results_df)





#---------4----------DCA_EfficientNetB0
# 导入必要库
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
from tensorflow.keras.mixed_precision import LossScaleOptimizer
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.profiler.model_analyzer import profile
from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
import time
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score,confusion_matrix
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 启用混合精度训练
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# 设置路径与参数
train_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/train"
val_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/val"
test_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/test"
input_shape = (224, 224, 3)
num_classes = 10
batch_size = 32
epochs = 35

# 定义自定义模块
class DynamicChannelAttention(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.global_avg_pool = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(self.reduction_ratio, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.fc2 = layers.Dense(input_shape[-1], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))

    def call(self, inputs):
        avg_pool = self.global_avg_pool(inputs)
        attention = self.fc1(avg_pool)
        attention = self.fc2(attention)
        attention = layers.Reshape((1, 1, -1))(attention)
        return inputs * attention

class SeparableDilatedConvBlock(layers.Layer):
    def __init__(self, filters, dilation_rate):
        super().__init__()
        self.conv1 = layers.SeparableConv2D(filters, (3, 3), dilation_rate=dilation_rate, padding='same', activation='swish')
        self.conv2 = layers.Conv2D(filters, (1, 1), padding='same', activation='swish')

    def call(self, inputs):
        x = self.conv1(inputs)
        return self.conv2(x)

class CANBlock(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(input_shape[-1] // self.reduction_ratio, kernel_size=(3, 3), padding='same', activation='swish')
        self.conv2 = layers.Conv2D(input_shape[-1], kernel_size=(1, 1), padding='same', activation='sigmoid')

    def call(self, inputs):
        x = self.conv1(inputs)
        attention_map = self.conv2(x)
        return inputs * attention_map

# 构建改进EfficientNetB0模型
def create_improved_efficientnet(input_shape=(224, 224, 3), num_classes=10, use_dca=True, use_sdc=True, use_can=True):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    base_model.trainable = True  # 微调
    model_input = layers.Input(shape=input_shape)
    x = base_model(model_input)
    if use_dca:
        x = DynamicChannelAttention()(x)
    if use_sdc:
        x = SeparableDilatedConvBlock(64, dilation_rate=(2, 2))(x)
    if use_can:
        x = CANBlock()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(1024, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # 输出层需显式指定为 float32
    model = models.Model(inputs=model_input, outputs=output)
    return model

# 消融实验的辅助函数：评估指标
def evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs):
    # 模型评估
    print(f"Evaluating {model_name}...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    
    # 精度、精确率、召回率、F1分数
    y_true = test_generator.classes
    y_pred = np.argmax(model.predict(test_generator), axis=-1)
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # 绘制混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()
    
    # 获取模型的FLOPs
    input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32)]  # 输入签名
    graph = tf.function(model).get_concrete_function(*input_signature)
    opts = ProfileOptionBuilder.float_operation()  # 配置FLOPs计算选项
    opts['output'] = 'none'
    flops = profile(graph.graph, options=opts).total_float_ops  # 获取FLOPs
    flops_g = flops / 1e9  # 将FLOPs换算成G FLOPs
    # 推理时间 (ms)
    start_time = time.time()
    # 进行多次推理，取平均时间
    n_iterations = 100  # 推理次数
    for _ in range(n_iterations):
        model.predict(test_generator, verbose=0)
    inference_time = (time.time() - start_time) / n_iterations 
    # 结果存储
    results = {
        'model_name': model_name,
        'accuracy': f"{test_acc * 100:.2f}%",
        'precision': f"{precision * 100:.2f}%",
        'recall': f"{recall * 100:.2f}%",
        'f1_score': f"{f1 * 100:.2f}%",
        'flops': flops_g,
        'params': model.count_params(),
        'inference_time': f"{inference_time:.2f}s",
    }
    # 创建结果表格
    results_df = pd.DataFrame([results])
    display(results_df)
    return results_df
    
# 设置消融实验的模型配置
def create_ablation_models(input_shape=(224， 224, 3), num_classes=10):
    models_config = {
        'DCA_EfficientNetB0': create_improved_efficientnet(input_shape=input_shape, num_classes=num_classes, use_sdc=False,use_can=False)   
    }
    return models_config

# 数据预处理
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)

reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.4, min_lr=1e-6, verbose=1)

# 获取消融实验的模型配置
models_config = create_ablation_models(input_shape=input_shape, num_classes=10)

# 训练并评估每个模型
historis = {}
results = {}
for model_name, model in models_config.items():
    print(f"Training model: {model_name}")
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='categorical_crossentropy'， 
                  metrics=['accuracy'])
    # 训练模型（使用对应的回调）
    history = model.fit(train_generator, 
                        epochs=epochs, 
                        validation_data=val_generator, 
                        callbacks=[reduce_lr])
    # 保存训练历史
    historis[model_name] = history.history
    results_df = evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs)
    results[model_name] = results_df
    '''
    # 获取测试集上的准确率
    test_accuracy = results_df.loc[0, 'accuracy']  # 从评估结果中提取测试集准确率
    test_accuracy = float(test_accuracy.strip('%')) / 100  # 将百分比转换为小数形式
    if model_name == 'DCA_SDC_CAN_EfficientNetB0'and test_accuracy <= 0.9950:
        print("Test accuracy is below threshold. Stopping training and exiting.")
        sys.exit()  # 退出程序
    '''
# 转换为 DataFrame，并展示
results_df = pd.concat(results.values(), ignore_index=True)
display(results_df)





#-----------5----------SDC_CAN_EfficientNetB0
# 导入必要库
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
from tensorflow.keras.mixed_precision import LossScaleOptimizer
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.profiler.model_analyzer import profile
from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
import time
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score,confusion_matrix
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 启用混合精度训练
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# 设置路径与参数
train_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/train"
val_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/val"
test_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/test"
input_shape = (224, 224, 3)
num_classes = 10
batch_size = 32
epochs = 35

# 定义自定义模块
class DynamicChannelAttention(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.global_avg_pool = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(self.reduction_ratio, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.fc2 = layers.Dense(input_shape[-1], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))

    def call(self, inputs):
        avg_pool = self.global_avg_pool(inputs)
        attention = self.fc1(avg_pool)
        attention = self.fc2(attention)
        attention = layers.Reshape((1, 1, -1))(attention)
        return inputs * attention

class SeparableDilatedConvBlock(layers.Layer):
    def __init__(self, filters, dilation_rate):
        super().__init__()
        self.conv1 = layers.SeparableConv2D(filters, (3, 3), dilation_rate=dilation_rate, padding='same', activation='swish')
        self.conv2 = layers.Conv2D(filters, (1, 1), padding='same', activation='swish')

    def call(self, inputs):
        x = self.conv1(inputs)
        return self.conv2(x)

class CANBlock(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(input_shape[-1] // self.reduction_ratio, kernel_size=(3, 3), padding='same', activation='swish')
        self.conv2 = layers.Conv2D(input_shape[-1], kernel_size=(1, 1), padding='same', activation='sigmoid')

    def call(self, inputs):
        x = self.conv1(inputs)
        attention_map = self.conv2(x)
        return inputs * attention_map

# 构建改进EfficientNetB0模型
def create_improved_efficientnet(input_shape=(224, 224, 3), num_classes=10, use_dca=True, use_sdc=True, use_can=True):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    base_model.trainable = True  # 微调
    model_input = layers.Input(shape=input_shape)
    x = base_model(model_input)
    if use_dca:
        x = DynamicChannelAttention()(x)
    if use_sdc:
        x = SeparableDilatedConvBlock(64, dilation_rate=(2, 2))(x)
    if use_can:
        x = CANBlock()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(1024, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # 输出层需显式指定为 float32
    model = models.Model(inputs=model_input, outputs=output)
    return model

# 消融实验的辅助函数：评估指标
def evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs):
    # 模型评估
    print(f"Evaluating {model_name}...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    
    # 精度、精确率、召回率、F1分数
    y_true = test_generator.classes
    y_pred = np.argmax(model.predict(test_generator), axis=-1)
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # 绘制混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()
    
    # 获取模型的FLOPs
    input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32)]  # 输入签名
    graph = tf.function(model).get_concrete_function(*input_signature)
    opts = ProfileOptionBuilder.float_operation()  # 配置FLOPs计算选项
    opts['output'] = 'none'
    flops = profile(graph.graph, options=opts).total_float_ops  # 获取FLOPs
    flops_g = flops / 1e9  # 将FLOPs换算成G FLOPs
    # 推理时间 (ms)
    start_time = time.time()
    # 进行多次推理，取平均时间
    n_iterations = 100  # 推理次数
    for _ in range(n_iterations):
        model.predict(test_generator, verbose=0)
    inference_time = (time.time() - start_time) / n_iterations 
    # 结果存储
    results = {
        'model_name': model_name,
        'accuracy': f"{test_acc * 100:.2f}%",
        'precision': f"{precision * 100:.2f}%",
        'recall': f"{recall * 100:.2f}%",
        'f1_score': f"{f1 * 100:.2f}%",
        'flops': flops_g,
        'params': model.count_params(),
        'inference_time': f"{inference_time:.2f}s",
    }
    # 创建结果表格
    results_df = pd.DataFrame([results])
    display(results_df)
    return results_df
    
# 设置消融实验的模型配置
def create_ablation_models(input_shape=(224， 224, 3), num_classes=10):
    models_config = {
        'SDC_CAN_EfficientNetB0': create_improved_efficientnet(input_shape=input_shape, num_classes=num_classes, use_dca=False)   
    }
    return models_config

# 数据预处理
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)

reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.4, min_lr=1e-6, verbose=1)

# 获取消融实验的模型配置
models_config = create_ablation_models(input_shape=input_shape, num_classes=10)

# 训练并评估每个模型
historis = {}
results = {}
for model_name, model in models_config.items():
    print(f"Training model: {model_name}")
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='categorical_crossentropy'， 
                  metrics=['accuracy'])
    # 训练模型（使用对应的回调）
    history = model.fit(train_generator, 
                        epochs=epochs, 
                        validation_data=val_generator, 
                        callbacks=[reduce_lr])
    # 保存训练历史
    historis[model_name] = history.history
    results_df = evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs)
    results[model_name] = results_df
    '''
    # 获取测试集上的准确率
    test_accuracy = results_df.loc[0, 'accuracy']  # 从评估结果中提取测试集准确率
    test_accuracy = float(test_accuracy.strip('%')) / 100  # 将百分比转换为小数形式
    if model_name == 'DCA_SDC_CAN_EfficientNetB0'and test_accuracy <= 0.9950:
        print("Test accuracy is below threshold. Stopping training and exiting.")
        sys.exit()  # 退出程序
    '''
# 转换为 DataFrame，并展示
results_df = pd.concat(results.values(), ignore_index=True)
display(results_df)





#----------6---------SDC_EfficientNetB0
# 导入必要库
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
from tensorflow.keras.mixed_precision import LossScaleOptimizer
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.profiler.model_analyzer import profile
from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
import time
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score,confusion_matrix
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 启用混合精度训练
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# 设置路径与参数
train_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/train"
val_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/val"
test_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/test"
input_shape = (224, 224, 3)
num_classes = 10
batch_size = 32
epochs = 35

# 定义自定义模块
class DynamicChannelAttention(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.global_avg_pool = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(self.reduction_ratio, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.fc2 = layers.Dense(input_shape[-1], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))

    def call(self, inputs):
        avg_pool = self.global_avg_pool(inputs)
        attention = self.fc1(avg_pool)
        attention = self.fc2(attention)
        attention = layers.Reshape((1, 1, -1))(attention)
        return inputs * attention

class SeparableDilatedConvBlock(layers.Layer):
    def __init__(self, filters, dilation_rate):
        super().__init__()
        self.conv1 = layers.SeparableConv2D(filters, (3, 3), dilation_rate=dilation_rate, padding='same', activation='swish')
        self.conv2 = layers.Conv2D(filters, (1, 1), padding='same', activation='swish')

    def call(self, inputs):
        x = self.conv1(inputs)
        return self.conv2(x)

class CANBlock(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(input_shape[-1] // self.reduction_ratio, kernel_size=(3, 3), padding='same', activation='swish')
        self.conv2 = layers.Conv2D(input_shape[-1], kernel_size=(1, 1), padding='same', activation='sigmoid')

    def call(self, inputs):
        x = self.conv1(inputs)
        attention_map = self.conv2(x)
        return inputs * attention_map

# 构建改进EfficientNetB0模型
def create_improved_efficientnet(input_shape=(224, 224, 3), num_classes=10, use_dca=True, use_sdc=True, use_can=True):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    base_model.trainable = True  # 微调
    model_input = layers.Input(shape=input_shape)
    x = base_model(model_input)
    if use_dca:
        x = DynamicChannelAttention()(x)
    if use_sdc:
        x = SeparableDilatedConvBlock(64, dilation_rate=(2, 2))(x)
    if use_can:
        x = CANBlock()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(1024, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # 输出层需显式指定为 float32
    model = models.Model(inputs=model_input, outputs=output)
    return model

# 消融实验的辅助函数：评估指标
def evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs):
    # 模型评估
    print(f"Evaluating {model_name}...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    
    # 精度、精确率、召回率、F1分数
    y_true = test_generator.classes
    y_pred = np.argmax(model.predict(test_generator), axis=-1)
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # 绘制混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()
    
    # 获取模型的FLOPs
    input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32)]  # 输入签名
    graph = tf.function(model).get_concrete_function(*input_signature)
    opts = ProfileOptionBuilder.float_operation()  # 配置FLOPs计算选项
    opts['output'] = 'none'
    flops = profile(graph.graph, options=opts).total_float_ops  # 获取FLOPs
    flops_g = flops / 1e9  # 将FLOPs换算成G FLOPs
    # 推理时间 (ms)
    start_time = time.time()
    # 进行多次推理，取平均时间
    n_iterations = 100  # 推理次数
    for _ in range(n_iterations):
        model.predict(test_generator, verbose=0)
    inference_time = (time.time() - start_time) / n_iterations 
    # 结果存储
    results = {
        'model_name': model_name,
        'accuracy': f"{test_acc * 100:.2f}%",
        'precision': f"{precision * 100:.2f}%",
        'recall': f"{recall * 100:.2f}%",
        'f1_score': f"{f1 * 100:.2f}%",
        'flops': flops_g,
        'params': model.count_params(),
        'inference_time': f"{inference_time:.2f}s",
    }
    # 创建结果表格
    results_df = pd.DataFrame([results])
    display(results_df)
    return results_df
    
# 设置消融实验的模型配置
def create_ablation_models(input_shape=(224， 224, 3), num_classes=10):
    models_config = {
        'SDC_EfficientNetB0': create_improved_efficientnet(input_shape=input_shape, num_classes=num_classes, use_dca=False,use_can=False)   
    }
    return models_config

# 数据预处理
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)

reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.4, min_lr=1e-6, verbose=1)

# 获取消融实验的模型配置
models_config = create_ablation_models(input_shape=input_shape, num_classes=10)

# 训练并评估每个模型
historis = {}
results = {}
for model_name, model in models_config.items():
    print(f"Training model: {model_name}")
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='categorical_crossentropy'， 
                  metrics=['accuracy'])
    # 训练模型（使用对应的回调）
    history = model.fit(train_generator, 
                        epochs=epochs, 
                        validation_data=val_generator, 
                        callbacks=[reduce_lr])
    # 保存训练历史
    historis[model_name] = history.history
    results_df = evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs)
    results[model_name] = results_df
    '''
    # 获取测试集上的准确率
    test_accuracy = results_df.loc[0, 'accuracy']  # 从评估结果中提取测试集准确率
    test_accuracy = float(test_accuracy.strip('%')) / 100  # 将百分比转换为小数形式
    if model_name == 'DCA_SDC_CAN_EfficientNetB0'and test_accuracy <= 0.9950:
        print("Test accuracy is below threshold. Stopping training and exiting.")
        sys.exit()  # 退出程序
    '''
# 转换为 DataFrame，并展示
results_df = pd.concat(results.values(), ignore_index=True)
display(results_df)





#----------7---------CAN_EfficientNetB0
# 导入必要库
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
from tensorflow.keras.mixed_precision import LossScaleOptimizer
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.profiler.model_analyzer import profile
from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
import time
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score,confusion_matrix
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 启用混合精度训练
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# 设置路径与参数
train_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/train"
val_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/val"
test_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/test"
input_shape = (224, 224, 3)
num_classes = 10
batch_size = 32
epochs = 35

# 定义自定义模块
class DynamicChannelAttention(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.global_avg_pool = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(self.reduction_ratio, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.fc2 = layers.Dense(input_shape[-1], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))

    def call(self, inputs):
        avg_pool = self.global_avg_pool(inputs)
        attention = self.fc1(avg_pool)
        attention = self.fc2(attention)
        attention = layers.Reshape((1, 1, -1))(attention)
        return inputs * attention

class SeparableDilatedConvBlock(layers.Layer):
    def __init__(self, filters, dilation_rate):
        super().__init__()
        self.conv1 = layers.SeparableConv2D(filters, (3, 3), dilation_rate=dilation_rate, padding='same', activation='swish')
        self.conv2 = layers.Conv2D(filters, (1, 1), padding='same', activation='swish')

    def call(self, inputs):
        x = self.conv1(inputs)
        return self.conv2(x)

class CANBlock(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(input_shape[-1] // self.reduction_ratio, kernel_size=(3, 3), padding='same', activation='swish')
        self.conv2 = layers.Conv2D(input_shape[-1], kernel_size=(1, 1), padding='same', activation='sigmoid')

    def call(self, inputs):
        x = self.conv1(inputs)
        attention_map = self.conv2(x)
        return inputs * attention_map

# 构建改进EfficientNetB0模型
def create_improved_efficientnet(input_shape=(224, 224, 3), num_classes=10, use_dca=True, use_sdc=True, use_can=True):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    base_model.trainable = True  # 微调
    model_input = layers.Input(shape=input_shape)
    x = base_model(model_input)
    if use_dca:
        x = DynamicChannelAttention()(x)
    if use_sdc:
        x = SeparableDilatedConvBlock(64, dilation_rate=(2, 2))(x)
    if use_can:
        x = CANBlock()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(1024, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # 输出层需显式指定为 float32
    model = models.Model(inputs=model_input, outputs=output)
    return model

# 消融实验的辅助函数：评估指标
def evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs):
    # 模型评估
    print(f"Evaluating {model_name}...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    
    # 精度、精确率、召回率、F1分数
    y_true = test_generator.classes
    y_pred = np.argmax(model.predict(test_generator), axis=-1)
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # 绘制混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()
    
    # 获取模型的FLOPs
    input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32)]  # 输入签名
    graph = tf.function(model).get_concrete_function(*input_signature)
    opts = ProfileOptionBuilder.float_operation()  # 配置FLOPs计算选项
    opts['output'] = 'none'
    flops = profile(graph.graph, options=opts).total_float_ops  # 获取FLOPs
    flops_g = flops / 1e9  # 将FLOPs换算成G FLOPs
    # 推理时间 (ms)
    start_time = time.time()
    # 进行多次推理，取平均时间
    n_iterations = 100  # 推理次数
    for _ in range(n_iterations):
        model.predict(test_generator, verbose=0)
    inference_time = (time.time() - start_time) / n_iterations 
    # 结果存储
    results = {
        'model_name': model_name,
        'accuracy': f"{test_acc * 100:.2f}%",
        'precision': f"{precision * 100:.2f}%",
        'recall': f"{recall * 100:.2f}%",
        'f1_score': f"{f1 * 100:.2f}%",
        'flops': flops_g,
        'params': model.count_params(),
        'inference_time': f"{inference_time:.2f}s",
    }
    # 创建结果表格
    results_df = pd.DataFrame([results])
    display(results_df)
    return results_df
    
# 设置消融实验的模型配置
def create_ablation_models(input_shape=(224， 224, 3), num_classes=10):
    models_config = {
        'CAN_EfficientNetB0': create_improved_efficientnet(input_shape=input_shape, num_classes=num_classes, use_dca=False,use_sdc=False)   
    }
    return models_config

# 数据预处理
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)

reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.4, min_lr=1e-6, verbose=1)

# 获取消融实验的模型配置
models_config = create_ablation_models(input_shape=input_shape, num_classes=10)

# 训练并评估每个模型
historis = {}
results = {}
for model_name, model in models_config.items():
    print(f"Training model: {model_name}")
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='categorical_crossentropy'， 
                  metrics=['accuracy'])
    # 训练模型（使用对应的回调）
    history = model.fit(train_generator, 
                        epochs=epochs, 
                        validation_data=val_generator, 
                        callbacks=[reduce_lr])
    # 保存训练历史
    historis[model_name] = history.history
    results_df = evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs)
    results[model_name] = results_df
    '''
    # 获取测试集上的准确率
    test_accuracy = results_df.loc[0, 'accuracy']  # 从评估结果中提取测试集准确率
    test_accuracy = float(test_accuracy.strip('%')) / 100  # 将百分比转换为小数形式
    if model_name == 'DCA_SDC_CAN_EfficientNetB0'and test_accuracy <= 0.9950:
        print("Test accuracy is below threshold. Stopping training and exiting.")
        sys.exit()  # 退出程序
    '''
# 转换为 DataFrame，并展示
results_df = pd.concat(results.values(), ignore_index=True)
display(results_df)





#------------8-------------EfficientNetB0模型
# 导入必要库
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
from tensorflow.keras.mixed_precision import LossScaleOptimizer
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
from tensorflow.python.profiler.model_analyzer import profile
from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
import time
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score,confusion_matrix
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 启用混合精度训练
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# 设置路径与参数
train_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/train"
val_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/val"
test_dir = "/root/.cache/kagglehub/datasets/tomgzg/split-tomato-data-4-224x224/versions/1/test"
input_shape = (224, 224, 3)
num_classes = 10
batch_size = 32
epochs = 35

# 定义自定义模块
class DynamicChannelAttention(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.global_avg_pool = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(self.reduction_ratio, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(1e-4))
        self.fc2 = layers.Dense(input_shape[-1], activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))

    def call(self, inputs):
        avg_pool = self.global_avg_pool(inputs)
        attention = self.fc1(avg_pool)
        attention = self.fc2(attention)
        attention = layers.Reshape((1, 1, -1))(attention)
        return inputs * attention

class SeparableDilatedConvBlock(layers.Layer):
    def __init__(self, filters, dilation_rate):
        super().__init__()
        self.conv1 = layers.SeparableConv2D(filters, (3, 3), dilation_rate=dilation_rate, padding='same', activation='swish')
        self.conv2 = layers.Conv2D(filters, (1, 1), padding='same', activation='swish')

    def call(self, inputs):
        x = self.conv1(inputs)
        return self.conv2(x)

class CANBlock(layers.Layer):
    def __init__(self, reduction_ratio=8):
        super().__init__()
        self.reduction_ratio = reduction_ratio

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(input_shape[-1] // self.reduction_ratio, kernel_size=(3, 3), padding='same', activation='swish')
        self.conv2 = layers.Conv2D(input_shape[-1], kernel_size=(1, 1), padding='same', activation='sigmoid')

    def call(self, inputs):
        x = self.conv1(inputs)
        attention_map = self.conv2(x)
        return inputs * attention_map

# 构建改进EfficientNetB0模型
def create_improved_efficientnet(input_shape=(224, 224, 3), num_classes=10, use_dca=True, use_sdc=True, use_can=True):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    base_model.trainable = True  # 微调
    model_input = layers.Input(shape=input_shape)
    x = base_model(model_input)
    if use_dca:
        x = DynamicChannelAttention()(x)
    if use_sdc:
        x = SeparableDilatedConvBlock(64, dilation_rate=(2, 2))(x)
    if use_can:
        x = CANBlock()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(1024, activation='swish')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    output = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # 输出层需显式指定为 float32
    model = models.Model(inputs=model_input, outputs=output)
    return model

# 消融实验的辅助函数：评估指标
def evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs):
    # 模型评估
    print(f"Evaluating {model_name}...")
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    
    # 精度、精确率、召回率、F1分数
    y_true = test_generator.classes
    y_pred = np.argmax(model.predict(test_generator), axis=-1)
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # 绘制混淆矩阵
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()
    
    # 获取模型的FLOPs
    input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32)]  # 输入签名
    graph = tf.function(model).get_concrete_function(*input_signature)
    opts = ProfileOptionBuilder.float_operation()  # 配置FLOPs计算选项
    opts['output'] = 'none'
    flops = profile(graph.graph, options=opts).total_float_ops  # 获取FLOPs
    flops_g = flops / 1e9  # 将FLOPs换算成G FLOPs
    # 推理时间 (ms)
    start_time = time.time()
    # 进行多次推理，取平均时间
    n_iterations = 100  # 推理次数
    for _ in range(n_iterations):
        model.predict(test_generator, verbose=0)
    inference_time = (time.time() - start_time) / n_iterations 
    # 结果存储
    results = {
        'model_name': model_name,
        'accuracy': f"{test_acc * 100:.2f}%",
        'precision': f"{precision * 100:.2f}%",
        'recall': f"{recall * 100:.2f}%",
        'f1_score': f"{f1 * 100:.2f}%",
        'flops': flops_g,
        'params': model.count_params(),
        'inference_time': f"{inference_time:.2f}s",
    }
    # 创建结果表格
    results_df = pd.DataFrame([results])
    display(results_df)
    return results_df
    
# 设置消融实验的模型配置
def create_ablation_models(input_shape=(224， 224, 3), num_classes=10):
    models_config = {
        'EfficientNetB0': create_improved_efficientnet(input_shape=input_shape, num_classes=num_classes,use_dca=False, use_sdc=False, use_can=False)    
    }
    return models_config

# 数据预处理
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)

reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.4, min_lr=1e-6, verbose=1)

# 获取消融实验的模型配置
models_config = create_ablation_models(input_shape=input_shape, num_classes=10)

# 训练并评估每个模型
historis = {}
results = {}
for model_name, model in models_config.items():
    print(f"Training model: {model_name}")
    model.summary()
    model.compile(optimizer=tf.keras.optimizers.Adam(), 
                  loss='categorical_crossentropy'， 
                  metrics=['accuracy'])
    # 训练模型（使用对应的回调）
    history = model.fit(train_generator, 
                        epochs=epochs, 
                        validation_data=val_generator, 
                        callbacks=[reduce_lr])
    # 保存训练历史
    historis[model_name] = history.history
    results_df = evaluate_model(model, train_generator, val_generator, test_generator, model_name, history, epochs)
    results[model_name] = results_df
    '''
    # 获取测试集上的准确率
    test_accuracy = results_df.loc[0, 'accuracy']  # 从评估结果中提取测试集准确率
    test_accuracy = float(test_accuracy.strip('%')) / 100  # 将百分比转换为小数形式
    if model_name == 'DCA_SDC_CAN_EfficientNetB0'and test_accuracy <= 0.9950:
        print("Test accuracy is below threshold. Stopping training and exiting.")
        sys.exit()  # 退出程序
    '''
# 转换为 DataFrame，并展示
results_df = pd.concat(results.values(), ignore_index=True)
display(results_df)
